{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "import undetected_chromedriver.v2 as uc\n",
    "from pyvirtualdisplay import Display\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import pipeline\n",
    "from neo4j import GraphDatabase\n",
    "import torch\n",
    "\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanekyuk/bert-uncased-keyword-extractor\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"yanekyuk/bert-uncased-keyword-extractor\"\n",
    ")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Extract keywords and construct them back from tokens\n",
    "    \"\"\"\n",
    "    result = list()\n",
    "    keyword = \"\"\n",
    "    for token in nlp(text):\n",
    "        if token[\"entity\"] == \"I-KEY\":\n",
    "            keyword += (\n",
    "                token[\"word\"][2:]\n",
    "                if token[\"word\"].startswith(\"##\")\n",
    "                else f\" {token['word']}\"\n",
    "            )\n",
    "        else:\n",
    "            if keyword:\n",
    "                result.append(keyword)\n",
    "            keyword = token[\"word\"]\n",
    "    # Add the last keyword\n",
    "    result.append(keyword)\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keywords(\n",
    "    \"\"\"\n",
    "Broadcom agreed to acquire cloud computing company VMware in a $61 billion (€57bn) cash-and stock deal.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings(\n",
    "    \"\"\"\n",
    "Web APIs are a huge opportunity to access and integrate data from any sources with your graph. Most of them provide the data in JSON format.\n",
    "\n",
    "The Load JSON procedures retrieve data from URLs or maps and turn it into map value(s) for Cypher to consume. Cypher has support for deconstructing nested documents with dot syntax, slices, UNWIND etc. so it is easy to turn nested data into graphs.\n",
    "\n",
    "Sources with multiple JSON objects (JSONL,JSON Lines) in a stream, like the streaming Twitter format or the Yelp Kaggle dataset, are also supported,\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            \n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            \n",
    "            link = link.split(\"#\")[0]\n",
    "            \n",
    "            link = link.split(\"?\")[0]\n",
    "            \n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "entry_url = \"https://neo4j.com/docs\"\n",
    "data = dict()\n",
    "visit_list = [entry_url]\n",
    "already_visited = []\n",
    "visited_links_count = 0  # Sayaç\n",
    "\n",
    "while visit_list and visited_links_count < 20:  # 100'e ulaşana kadar devam et\n",
    "    \n",
    "    current_url = visit_list.pop()\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "    print(current_url)\n",
    "    try:\n",
    "        wd.get(current_url)\n",
    "    except:\n",
    "        print(f\"Couldn't open {current_url}\")\n",
    "        already_visited.append(current_url)\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        actual_url = wd.current_url.rstrip(\"/\").split(\"#\")[0].split(\"?\")[0]\n",
    "        if actual_url != current_url:\n",
    "            \n",
    "            data[current_url] = {\n",
    "                \"links\": [],\n",
    "                \"text\": None,\n",
    "                \"embeddings\": [],\n",
    "                \"keywords\": [],\n",
    "                \"redirects\": [actual_url],\n",
    "            }\n",
    "            already_visited.append(current_url)\n",
    "            \n",
    "            current_url = actual_url\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    text = extract_text_by_class(\"content\")\n",
    "    \n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"article\")\n",
    "   \n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"page\")\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"single-user-story\")\n",
    "    \n",
    "    try:\n",
    "        if \"Sorry, page not found\" in wd.find_element(By.TAG_NAME, \"body\").text:\n",
    "            text = \"404\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    if text:\n",
    "        embeddings = generate_embeddings(text)\n",
    "        keywords = extract_keywords(text)\n",
    "    else:\n",
    "        embeddings = []\n",
    "        keywords = []\n",
    "\n",
    " \n",
    "    links = extract_links_by_xpath(\"//div[@class='content']//a[@href]\")\n",
    "    \n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article[@class='article']//a[@href]\")\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article//a[@href]\")\n",
    "\n",
    "   \n",
    "    data[current_url] = {\n",
    "        \"links\": [l for l in links if l != current_url],\n",
    "        \"text\": text,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"keywords\": keywords,\n",
    "        \"redirects\": [],\n",
    "    }\n",
    "    \n",
    "    already_visited.append(current_url)\n",
    "    visited_links_count += 1  \n",
    "    visit_list.extend(\n",
    "        [\n",
    "            l\n",
    "            for l in list(links)\n",
    "            if (\"neo4j.com\" in l)\n",
    "            and (not l in already_visited)\n",
    "            and (not \"community.neo4j.com\" in l)\n",
    "            and (not \"sandbox.neo4j.com\" in l)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "wd.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neo4j bağlantısı kontrol\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 1\")\n",
    "        for record in result:\n",
    "            print(record)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yalnızca GDS ile çalıştır\n",
    "\n",
    "import json\n",
    "\n",
    "j = json.dumps(data)\n",
    "\n",
    "f = open(\"neo4j_docs.json\", \"w\")\n",
    "\n",
    "\n",
    "f.write(j)\n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "host = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "gds = GraphDataScience(host, auth=(user, password))\n",
    "\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT IF NOT EXISTS FOR (p:Page) REQUIRE p.url IS UNIQUE;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yalnızca GDM ile çalıştır\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "import json\n",
    "\n",
    "j = json.dumps(data)\n",
    "\n",
    "f = open(\"neo4j_docs.json\", \"w\")\n",
    "\n",
    "\n",
    "f.write(j)\n",
    "\n",
    "\n",
    "f.close()\n",
    "\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "\n",
    "with driver.session() as session:\n",
    "   \n",
    "    j = json.dumps(data)\n",
    "    with open(\"neo4j_docs.json\", \"w\") as f:\n",
    "        f.write(j)\n",
    "\n",
    "    \n",
    "    session.run(\"\"\"\n",
    "        CREATE CONSTRAINT IF NOT EXISTS FOR (p:Page) REQUIRE p.url IS UNIQUE;\n",
    "    \"\"\")\n",
    "    \n",
    "    session.run(\"\"\"\n",
    "        CREATE CONSTRAINT IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yalnızca gds ile çalıştır\n",
    "\n",
    "import_query = \"\"\"\n",
    "\n",
    "UNWIND $data AS row\n",
    "MERGE (p:Page {url:row.url})\n",
    "SET p.embedding = row.embedding,\n",
    "    p.has_text = row.has_text,\n",
    "    p.is_404 = row.is_404\n",
    "FOREACH (l in row.links    | MERGE (p1:Page {url:l}) MERGE (p)-[:LINKS_TO]->(p1))\n",
    "FOREACH (k in row.keywords | MERGE (k1:Keyword {name:k}) MERGE (p)-[:HAS_KEYWORD]->(k1))\n",
    "FOREACH (r in row.redirects| MERGE (r1:Page {url:r}) MERGE (p)-[:REDIRECTS]->(r1))\n",
    "\n",
    "\"\"\"\n",
    "x = 1\n",
    "params = []\n",
    "for key in data:\n",
    "    params.append(\n",
    "        {\n",
    "            \"url\": key,\n",
    "            \"embedding\": data[key][\"embeddings\"],\n",
    "            \"keywords\": data[key][\"keywords\"],\n",
    "            \"links\": data[key][\"links\"],\n",
    "            \"has_text\": True if data[key][\"text\"] else False,\n",
    "            \"is_404\": True if data[key][\"text\"] == \"404\" else False,\n",
    "            \"redirects\": data[key][\"redirects\"],\n",
    "        }\n",
    "    ),\n",
    "    \n",
    "    if len(params) == 500:\n",
    "        gds.run_cypher(import_query, {\"data\": params})\n",
    "        params = []\n",
    "        # Logging\n",
    "        print(f\"Importing {x} batch\")\n",
    "        x += 1\n",
    "\n",
    "gds.run_cypher(import_query, {\"data\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# yalnızca GDM ile çalıştır\n",
    "\n",
    "import json\n",
    "\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (p:Page {url:row.url})\n",
    "SET p.embedding = row.embedding,\n",
    "    p.has_text = row.has_text,\n",
    "    p.is_404 = row.is_404\n",
    "FOREACH (l in row.links    | MERGE (p1:Page {url:l}) MERGE (p)-[:LINKS_TO]->(p1))\n",
    "FOREACH (k in row.keywords | MERGE (k1:Keyword {name:k}) MERGE (p)-[:HAS_KEYWORD]->(k1))\n",
    "FOREACH (r in row.redirects| MERGE (r1:Page {url:r}) MERGE (p)-[:REDIRECTS]->(r1))\n",
    "\"\"\"\n",
    "\n",
    "x = 1\n",
    "params = []\n",
    "\n",
    "for key in data:\n",
    "    params.append(\n",
    "        {\n",
    "            \"url\": key,\n",
    "            \"embedding\": data[key][\"embeddings\"],\n",
    "            \"keywords\": data[key][\"keywords\"],\n",
    "            \"links\": data[key][\"links\"],\n",
    "            \"has_text\": True if data[key][\"text\"] else False,\n",
    "            \"is_404\": True if data[key][\"text\"] == \"404\" else False,\n",
    "            \"redirects\": data[key][\"redirects\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    \n",
    "    if len(params) == 500:\n",
    "        with driver.session() as session:\n",
    "            session.run(import_query, {\"data\": params})\n",
    "            params = []\n",
    "            # Logging\n",
    "            print(f\"Importing {x} batch\")\n",
    "            x += 1\n",
    "\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(import_query, {\"data\": params})\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "CALL apoc.meta.stats()\n",
    "YIELD labels, relTypesCount\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"labels\"], record[\"relTypesCount\"])\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)\n",
    "RETURN p.has_text AS has_text,\n",
    "       count(*) AS count\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"has_text\"], record[\"count\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)\n",
    "WHERE p.has_text IS NULL\n",
    "WITH p, [(p)<-[:LINKS_TO|REDIRECTS]-() | 1] AS links\n",
    "RETURN p.url AS page, size(links) AS links_count\n",
    "ORDER BY links_count DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"page\"], record[\"links_count\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDS version\n",
    "# \n",
    "#  gds.run_cypher(\"\"\"\n",
    "#  MATCH (:Page)-[:LINKS_TO|REDIRECTS]->(:Page{is_404:true})\n",
    "#  RETURN count(*) AS brokenLinkCount\n",
    "#  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (:Page)-[:LINKS_TO|REDIRECTS]->(:Page{is_404:true})\n",
    "RETURN count(*) AS brokenLinkCount\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"brokenLinkCount\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from graphdatascience import GraphProjector\n",
    "\n",
    "# GDS grafiğini oluştur\n",
    "# graph_projector = GraphProjector(host, auth=(user, password))\n",
    "# G = graph_projector.project_graph(\n",
    "#    graph=\"structure\",\n",
    "#    node_label=\"Page\",\n",
    "#    relationship_types=[\"LINKS_TO\", \"REDIRECTS\"]\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ****** GDS İLE PAGERANK ÖNEMLİ ****\n",
    "\n",
    "#from graphdatascience import PageRank\n",
    "\n",
    "# PageRank skorlarını hesapla\n",
    "#pagerank = PageRank()\n",
    "#pr_df = pagerank.fit_transform(G)\n",
    "\n",
    "# Skor sütununu \"pagerank\" olarak yeniden adlandır\n",
    "#pr_df.rename(columns={\"score\": \"pagerank\"}, inplace=True)\n",
    "\n",
    "# DataFrame'leri birleştir\n",
    "#combined_df = df.merge(pr_df, on=\"nodeId\")\n",
    "\n",
    "# PageRank'a göre sırala\n",
    "#combined_df.sort_values(\"pagerank\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "RETURN p.url AS page,\n",
    "       COLLECT(k.name) AS keywords\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"page\"], record[\"keywords\"])\n",
    "\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
