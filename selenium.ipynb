{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium==4.7.2 transformers==4.25.1 sentence-transformers===2.2.2 graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"python programlama dili\"\n",
    "x = x[-6:-18:-5]\n",
    "print(x)\n",
    "print(\"{1}{0}{2}{4}\".format(5,4,1,2,3))\n",
    "from functools import reduce\n",
    "f=lambda a1,a2:a1*a2\n",
    "s=reduce(f,range(5))\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt install chromium-chromedriver\n",
    "#!apt install -y xvfb\n",
    "\n",
    "%pip install undetected-chromedriver==3.1.7\n",
    "%pip install PyVirtualDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install undetected-chromedriver==3.1.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver.v2 as uc\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade jupyter ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanekyuk/bert-uncased-keyword-extractor\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"yanekyuk/bert-uncased-keyword-extractor\"\n",
    ")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Extract keywords and construct them back from tokens\n",
    "    \"\"\"\n",
    "    result = list()\n",
    "    keyword = \"\"\n",
    "    for token in nlp(text):\n",
    "        if token[\"entity\"] == \"I-KEY\":\n",
    "            keyword += (\n",
    "                token[\"word\"][2:]\n",
    "                if token[\"word\"].startswith(\"##\")\n",
    "                else f\" {token['word']}\"\n",
    "            )\n",
    "        else:\n",
    "            if keyword:\n",
    "                result.append(keyword)\n",
    "            keyword = token[\"word\"]\n",
    "    # Add the last keyword\n",
    "    result.append(keyword)\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keywords(\n",
    "    \"\"\"\n",
    "Broadcom agreed to acquire cloud computing company VMware in a $61 billion (â‚¬57bn) cash-and stock deal.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings(\n",
    "    \"\"\"\n",
    "Web APIs are a huge opportunity to access and integrate data from any sources with your graph. Most of them provide the data in JSON format.\n",
    "\n",
    "The Load JSON procedures retrieve data from URLs or maps and turn it into map value(s) for Cypher to consume. Cypher has support for deconstructing nested documents with dot syntax, slices, UNWIND etc. so it is easy to turn nested data into graphs.\n",
    "\n",
    "Sources with multiple JSON objects (JSONL,JSON Lines) in a stream, like the streaming Twitter format or the Yelp Kaggle dataset, are also supported,\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "\n",
    "entry_url = \"https://neo4j.com/docs\"\n",
    "data = dict()\n",
    "visit_list = [entry_url]\n",
    "already_visited = []\n",
    "\n",
    "while visit_list:\n",
    "    # Visit the URL\n",
    "    current_url = visit_list.pop()\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "    print(current_url)\n",
    "    try:\n",
    "        wd.get(current_url)\n",
    "    except:\n",
    "        print(f\"Couldn't open {current_url}\")\n",
    "        already_visited.append(current_url)\n",
    "        continue\n",
    "    # If redirect\n",
    "    try:\n",
    "        actual_url = wd.current_url.rstrip(\"/\").split(\"#\")[0].split(\"?\")[0]\n",
    "        if actual_url != current_url:\n",
    "            # Store the redirect information\n",
    "            data[current_url] = {\n",
    "                \"links\": [],\n",
    "                \"text\": None,\n",
    "                \"embeddings\": [],\n",
    "                \"keywords\": [],\n",
    "                \"redirects\": [actual_url],\n",
    "            }\n",
    "            already_visited.append(current_url)\n",
    "            # Content is being extracted from the actual url\n",
    "            current_url = actual_url\n",
    "    except:\n",
    "        pass\n",
    "    # Extract text from the content div\n",
    "    text = extract_text_by_class(\"content\")\n",
    "    # If nothing is found, try article div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"article\")\n",
    "    # If nothing is found, try page div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"page\")\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"single-user-story\")\n",
    "    # Check if 404\n",
    "    try:\n",
    "        if \"Sorry, page not found\" in wd.find_element(By.TAG_NAME, \"body\").text:\n",
    "            text = \"404\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Generate paragraph embedding & extract keywords\n",
    "    if text:\n",
    "        embeddings = generate_embeddings(text)\n",
    "        keywords = extract_keywords(text)\n",
    "    else:\n",
    "        embeddings = []\n",
    "        keywords = []\n",
    "\n",
    "    # Extract links from the content div\n",
    "    links = extract_links_by_xpath(\"//div[@class='content']//a[@href]\")\n",
    "    # If nothing is found, try article div\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article[@class='article']//a[@href]\")\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article//a[@href]\")\n",
    "\n",
    "    # if not links and not text:\n",
    "    #    print(f\"Couldn't retrieve the data from {current_url}\")\n",
    "    # Store page information\n",
    "    data[current_url] = {\n",
    "        \"links\": [l for l in links if l != current_url],\n",
    "        \"text\": text,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"keywords\": keywords,\n",
    "        \"redirects\": [],\n",
    "    }\n",
    "    # Crawling information\n",
    "    already_visited.append(current_url)\n",
    "    # Don't leave neo4j.com while crawling\n",
    "    # and avoid community and sandbox\n",
    "    visit_list.extend(\n",
    "        [\n",
    "            l\n",
    "            for l in list(links)\n",
    "            if (\"neo4j.com\" in l)\n",
    "            and (not l in already_visited)\n",
    "            and (not \"community.neo4j.com\" in l)\n",
    "            and (not \"sandbox.neo4j.com\" in l)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # service.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "# Set up the SentenceTransformer model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "entry_url = \"https://neo4j.com/docs\"\n",
    "data = dict()\n",
    "visit_list = [entry_url]\n",
    "already_visited = []\n",
    "visited_links_count = 0  # SayaÃ§\n",
    "\n",
    "while visit_list and visited_links_count < 20:  # 100'e ulaÅŸana kadar devam et\n",
    "    # Visit the URL\n",
    "    current_url = visit_list.pop()\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "    print(current_url)\n",
    "    try:\n",
    "        wd.get(current_url)\n",
    "    except:\n",
    "        print(f\"Couldn't open {current_url}\")\n",
    "        already_visited.append(current_url)\n",
    "        continue\n",
    "    # If redirect\n",
    "    try:\n",
    "        actual_url = wd.current_url.rstrip(\"/\").split(\"#\")[0].split(\"?\")[0]\n",
    "        if actual_url != current_url:\n",
    "            # Store the redirect information\n",
    "            data[current_url] = {\n",
    "                \"links\": [],\n",
    "                \"text\": None,\n",
    "                \"embeddings\": [],\n",
    "                \"keywords\": [],\n",
    "                \"redirects\": [actual_url],\n",
    "            }\n",
    "            already_visited.append(current_url)\n",
    "            # Content is being extracted from the actual url\n",
    "            current_url = actual_url\n",
    "    except:\n",
    "        pass\n",
    "    # Extract text from the content div\n",
    "    text = extract_text_by_class(\"content\")\n",
    "    # If nothing is found, try article div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"article\")\n",
    "    # If nothing is found, try page div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"page\")\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"single-user-story\")\n",
    "    # Check if 404\n",
    "    try:\n",
    "        if \"Sorry, page not found\" in wd.find_element(By.TAG_NAME, \"body\").text:\n",
    "            text = \"404\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Generate paragraph embedding & extract keywords\n",
    "    if text:\n",
    "        embeddings = generate_embeddings(text)\n",
    "        keywords = extract_keywords(text)\n",
    "    else:\n",
    "        embeddings = []\n",
    "        keywords = []\n",
    "\n",
    "    # Extract links from the content div\n",
    "    links = extract_links_by_xpath(\"//div[@class='content']//a[@href]\")\n",
    "    # If nothing is found, try article div\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article[@class='article']//a[@href]\")\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article//a[@href]\")\n",
    "\n",
    "    # if not links and not text:\n",
    "    #    print(f\"Couldn't retrieve the data from {current_url}\")\n",
    "    # Store page information\n",
    "    data[current_url] = {\n",
    "        \"links\": [l for l in links if l != current_url],\n",
    "        \"text\": text,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"keywords\": keywords,\n",
    "        \"redirects\": [],\n",
    "    }\n",
    "    # Crawling information\n",
    "    already_visited.append(current_url)\n",
    "    visited_links_count += 1  # Her ziyarette sayaÃ§ arttÄ±rÄ±lÄ±yor\n",
    "    # Don't leave neo4j.com while crawling\n",
    "    # and avoid community and sandbox\n",
    "    visit_list.extend(\n",
    "        [\n",
    "            l\n",
    "            for l in list(links)\n",
    "            if (\"neo4j.com\" in l)\n",
    "            and (not l in already_visited)\n",
    "            and (not \"community.neo4j.com\" in l)\n",
    "            and (not \"sandbox.neo4j.com\" in l)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Close the webdriver\n",
    "wd.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://5a2c45ba.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"cMVJrBii27ZJMgsEBm5gw-uE7e2rGXnRHk1MFLFlu5c\"\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 1\")\n",
    "        for record in result:\n",
    "            print(record)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "j = json.dumps(data)\n",
    "# open file for writing, \"w\"\n",
    "f = open(\"neo4j_docs.json\", \"w\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(j)\n",
    "\n",
    "# close file\n",
    "f.close()\n",
    "# Import to Neo4j\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "host = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "gds = GraphDataScience(host, auth=(user, password))\n",
    "\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT IF NOT EXISTS FOR (p:Page) REQUIRE p.url IS UNIQUE;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± oluÅŸtur\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# VeritabanÄ±na yazma iÅŸlemlerini gerÃ§ekleÅŸtir\n",
    "with driver.session() as session:\n",
    "    # Json verilerini dosyaya yaz\n",
    "    j = json.dumps(data)\n",
    "    with open(\"neo4j_docs.json\", \"w\") as f:\n",
    "        f.write(j)\n",
    "\n",
    "    # KÄ±sÄ±tlamalarÄ± oluÅŸtur\n",
    "    session.run(\"\"\"\n",
    "        CREATE CONSTRAINT IF NOT EXISTS FOR (p:Page) REQUIRE p.url IS UNIQUE;\n",
    "    \"\"\")\n",
    "    \n",
    "    session.run(\"\"\"\n",
    "        CREATE CONSTRAINT IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "# VeritabanÄ± baÄŸlantÄ±sÄ±nÄ± kapat\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "\n",
    "UNWIND $data AS row\n",
    "MERGE (p:Page {url:row.url})\n",
    "SET p.embedding = row.embedding,\n",
    "    p.has_text = row.has_text,\n",
    "    p.is_404 = row.is_404\n",
    "FOREACH (l in row.links    | MERGE (p1:Page {url:l}) MERGE (p)-[:LINKS_TO]->(p1))\n",
    "FOREACH (k in row.keywords | MERGE (k1:Keyword {name:k}) MERGE (p)-[:HAS_KEYWORD]->(k1))\n",
    "FOREACH (r in row.redirects| MERGE (r1:Page {url:r}) MERGE (p)-[:REDIRECTS]->(r1))\n",
    "\n",
    "\"\"\"\n",
    "x = 1\n",
    "params = []\n",
    "for key in data:\n",
    "    params.append(\n",
    "        {\n",
    "            \"url\": key,\n",
    "            \"embedding\": data[key][\"embeddings\"],\n",
    "            \"keywords\": data[key][\"keywords\"],\n",
    "            \"links\": data[key][\"links\"],\n",
    "            \"has_text\": True if data[key][\"text\"] else False,\n",
    "            \"is_404\": True if data[key][\"text\"] == \"404\" else False,\n",
    "            \"redirects\": data[key][\"redirects\"],\n",
    "        }\n",
    "    )\n",
    "    # Batch per 500\n",
    "    if len(params) == 500:\n",
    "        gds.run_cypher(import_query, {\"data\": params})\n",
    "        params = []\n",
    "        # Logging\n",
    "        print(f\"Importing {x} batch\")\n",
    "        x += 1\n",
    "\n",
    "# Import the remainder\n",
    "gds.run_cypher(import_query, {\"data\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Import to Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (p:Page {url:row.url})\n",
    "SET p.embedding = row.embedding,\n",
    "    p.has_text = row.has_text,\n",
    "    p.is_404 = row.is_404\n",
    "FOREACH (l in row.links    | MERGE (p1:Page {url:l}) MERGE (p)-[:LINKS_TO]->(p1))\n",
    "FOREACH (k in row.keywords | MERGE (k1:Keyword {name:k}) MERGE (p)-[:HAS_KEYWORD]->(k1))\n",
    "FOREACH (r in row.redirects| MERGE (r1:Page {url:r}) MERGE (p)-[:REDIRECTS]->(r1))\n",
    "\"\"\"\n",
    "\n",
    "x = 1\n",
    "params = []\n",
    "\n",
    "for key in data:\n",
    "    params.append(\n",
    "        {\n",
    "            \"url\": key,\n",
    "            \"embedding\": data[key][\"embeddings\"],\n",
    "            \"keywords\": data[key][\"keywords\"],\n",
    "            \"links\": data[key][\"links\"],\n",
    "            \"has_text\": True if data[key][\"text\"] else False,\n",
    "            \"is_404\": True if data[key][\"text\"] == \"404\" else False,\n",
    "            \"redirects\": data[key][\"redirects\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Batch per 500\n",
    "    if len(params) == 500:\n",
    "        with driver.session() as session:\n",
    "            session.run(import_query, {\"data\": params})\n",
    "            params = []\n",
    "            # Logging\n",
    "            print(f\"Importing {x} batch\")\n",
    "            x += 1\n",
    "\n",
    "# Import the remainder\n",
    "with driver.session() as session:\n",
    "    session.run(import_query, {\"data\": params})\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "CALL apoc.meta.stats()\n",
    "YIELD labels, relTypesCount\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"labels\"], record[\"relTypesCount\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)\n",
    "RETURN p.has_text AS has_text,\n",
    "       count(*) AS count\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"has_text\"], record[\"count\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)\n",
    "WHERE p.has_text IS NULL\n",
    "WITH p, [(p)<-[:LINKS_TO|REDIRECTS]-() | 1] AS links\n",
    "RETURN p.url AS page, size(links) AS links_count\n",
    "ORDER BY links_count DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"page\"], record[\"links_count\"])\n",
    "\n",
    "driver.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDS version\n",
    "# \n",
    "#  gds.run_cypher(\"\"\"\n",
    "#  MATCH (:Page)-[:LINKS_TO|REDIRECTS]->(:Page{is_404:true})\n",
    "#  RETURN count(*) AS brokenLinkCount\n",
    "#  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (:Page)-[:LINKS_TO|REDIRECTS]->(:Page{is_404:true})\n",
    "RETURN count(*) AS brokenLinkCount\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"brokenLinkCount\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (start:Page {url:\"https://neo4j.com/docs\"}), \n",
    "      (end:Page {url:\"https://console.neo4j.io\"})\n",
    "MATCH p=shortestPath((start)-[:LINKS_TO|REDIRECTS*..10]->(end))\n",
    "RETURN [n in nodes(p) | n.url] AS path\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"path\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install GraphProjector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphProjector\n",
    "\n",
    "# GDS grafiÄŸini oluÅŸtur\n",
    "graph_projector = GraphProjector(host, auth=(user, password))\n",
    "G = graph_projector.project_graph(\n",
    "    graph=\"structure\",\n",
    "    node_label=\"Page\",\n",
    "    relationship_types=[\"LINKS_TO\", \"REDIRECTS\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphdatascience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import Degree\n",
    "\n",
    "# Ters yÃ¶nlÃ¼ dereceleri hesapla\n",
    "degree = Degree()\n",
    "df = degree.fit_transform(G, orientation=\"REVERSE\")\n",
    "\n",
    "# Her dÃ¼ÄŸÃ¼mÃ¼n URL'sini al\n",
    "df[\"url\"] = [node[\"url\"] for node in gds.util.asNodes(df[\"nodeId\"].tolist())]\n",
    "\n",
    "# Dereceye gÃ¶re DataFrame'i sÄ±rala\n",
    "df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "\n",
    "# Ä°lk beÅŸ Ã¶ÄŸeyi gÃ¶ster\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (n:Page)\n",
    "RETURN n.url AS url, size((n)<-[:LINKS_TO]-()) AS inDegree\n",
    "ORDER BY inDegree DESC\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"url\"], record[\"inDegree\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import PageRank\n",
    "\n",
    "# PageRank skorlarÄ±nÄ± hesapla\n",
    "pagerank = PageRank()\n",
    "pr_df = pagerank.fit_transform(G)\n",
    "\n",
    "# Skor sÃ¼tununu \"pagerank\" olarak yeniden adlandÄ±r\n",
    "pr_df.rename(columns={\"score\": \"pagerank\"}, inplace=True)\n",
    "\n",
    "# DataFrame'leri birleÅŸtir\n",
    "combined_df = df.merge(pr_df, on=\"nodeId\")\n",
    "\n",
    "# PageRank'a gÃ¶re sÄ±rala\n",
    "combined_df.sort_values(\"pagerank\", ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.pageRank.stream('structure') YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).url AS url, score\n",
    "ORDER BY score DESC\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"url\"], record[\"score\"])\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (k:Keyword)\n",
    "RETURN k.name AS keyword,\n",
    "       COUNT(*) AS mentions\n",
    "ORDER BY mentions DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"keyword\"], record[\"mentions\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, metadata = gds.graph.project(\n",
    "    \"keywords\", [\"Page\", \"Keyword\"], {\"HAS_KEYWORD\": {\"orientation\": \"REVERSE\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "RETURN p.url AS page,\n",
    "       COLLECT(k.name) AS keywords\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"page\"], record[\"keywords\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Page), (b:Page)\n",
    "WHERE id(a) < id(b) AND NOT (a)-[:CO_OCCUR]->(b)\n",
    "WITH a, b, gds.alpha.similarity.jaccard([a, b], {relationshipQuery: \"CO_OCCUR\"}) AS similarity\n",
    "WHERE similarity > 0.4\n",
    "MERGE (a)-[r:CO_OCCUR {score: similarity}]->(b)\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(query)\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Page), (b:Page)\n",
    "WHERE id(a) < id(b)\n",
    "WITH a, b,\n",
    "     algo.similarity.jaccard(apoc.coll.toSet(COLLECT(id(a))), apoc.coll.toSet(COLLECT(id(b)))) AS similarity\n",
    "WHERE similarity > 0.4 AND NOT (a)-[:CO_OCCUR]->(b)\n",
    "MERGE (a)-[r:CO_OCCUR {score: similarity}]->(b)\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(query)\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = gds.louvain.stream(G, nodeLabels=[\"Keyword\"], relationshipTypes=[\"CO_OCCUR\"])\n",
    "topic_df[\"keyword\"] = [\n",
    "    n[\"name\"] for n in gds.util.asNodes(topic_df[\"nodeId\"].to_list())\n",
    "]\n",
    "topic_df.groupby(\"communityId\").agg(\n",
    "    {\"keyword\": [\"size\", list]}\n",
    ").reset_index().sort_values([(\"keyword\", \"size\")], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = gds.louvain.stream(G, tolerance=(0.00001), nodeLabels=[\"Keyword\"], relationshipTypes=[\"CO_OCCUR\"])\n",
    "topic_df[\"keyword\"] = [\n",
    "    n[\"name\"] for n in gds.util.asNodes(topic_df[\"nodeId\"].to_list())\n",
    "]\n",
    "topic_df.groupby(\"communityId\").agg(\n",
    "    {\"keyword\": [\"size\", list]}\n",
    ").reset_index().sort_values([(\"keyword\", \"size\")], ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Create a network (you can replace this with your actual network data)\n",
    "graph = {\n",
    "  0: [1, 2],\n",
    "  1: [0, 2, 3],\n",
    "  2: [0, 1, 3, 4],\n",
    "  3: [1, 2, 4],\n",
    "  4: [2, 3]\n",
    "}\n",
    "\n",
    "# Detect communities using Louvain method\n",
    "partition = community.best_partition(graph)\n",
    "\n",
    "# Print the communities\n",
    "for node, comm in partition.items():\n",
    "  print(f\"Node {node} belongs to community {comm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Neo4j veritabanÄ±na baÄŸlan\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"  # BaÄŸlanmak istediÄŸiniz veritabanÄ±nÄ±n URI'si\n",
    "username = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "# Oturumu oluÅŸtur\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "def louvain_community_detection(session):\n",
    "    # Topluluk algÄ±sÄ±nÄ± gerÃ§ekleÅŸtirme sorgusu\n",
    "    query = \"\"\"\n",
    "    CALL gds.louvain.stream('myGraph') \n",
    "    YIELD nodeId, communityId\n",
    "    RETURN gds.util.asNode(nodeId).name AS name, communityId\n",
    "    \"\"\"\n",
    "    result = session.run(query)\n",
    "    community_data = {record[\"name\"]: record[\"communityId\"] for record in result}\n",
    "    return community_data\n",
    "\n",
    "def convert_node_ids_to_keywords(session, community_data):\n",
    "    # Anahtar kelime kimliklerini anahtar kelime adlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼rme sorgusu\n",
    "    query = \"\"\"\n",
    "    MATCH (k:Keyword)\n",
    "    WHERE id(k) IN $nodeIds\n",
    "    RETURN k.name AS name\n",
    "    \"\"\"\n",
    "    node_ids = [int(node_id) for node_id in community_data.keys()]\n",
    "    result = session.run(query, {\"nodeIds\": node_ids})\n",
    "    keyword_data = {record[\"name\"]: community_data[str(node_id)] for record, node_id in zip(result, node_ids)}\n",
    "    return keyword_data\n",
    "\n",
    "def analyze_communities(community_data, keyword_data):\n",
    "    # Topluluk analizini gerÃ§ekleÅŸtirme\n",
    "    community_keyword_count = {}\n",
    "    for keyword, community_id in keyword_data.items():\n",
    "        if community_id not in community_keyword_count:\n",
    "            community_keyword_count[community_id] = {\"size\": 0, \"keywords\": []}\n",
    "        community_keyword_count[community_id][\"size\"] += 1\n",
    "        community_keyword_count[community_id][\"keywords\"].append(keyword)\n",
    "    return community_keyword_count\n",
    "\n",
    "# Grafa eriÅŸim saÄŸla ve iÅŸlemleri gerÃ§ekleÅŸtir\n",
    "with driver.session() as session:\n",
    "    # Topluluk algÄ±sÄ±nÄ± gerÃ§ekleÅŸtir\n",
    "    community_data = louvain_community_detection(session)\n",
    "\n",
    "    # Anahtar kelime kimliklerini anahtar kelime adlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼r\n",
    "    keyword_data = convert_node_ids_to_keywords(session, community_data)\n",
    "\n",
    "    # Topluluk analizini gerÃ§ekleÅŸtir\n",
    "    community_keyword_count = analyze_communities(community_data, keyword_data)\n",
    "\n",
    "    # SonuÃ§larÄ± iÅŸle\n",
    "    sorted_communities = sorted(community_keyword_count.items(), key=lambda x: x[1][\"size\"], reverse=True)\n",
    "    top_communities = sorted_communities[:5]  # Ä°lk 5 bÃ¼yÃ¼k topluluÄŸu al\n",
    "\n",
    "    # TopluluklarÄ± ve iÃ§erdiÄŸi anahtar kelimeleri yazdÄ±r\n",
    "    for community_id, data in top_communities:\n",
    "        print(f\"Topluluk {community_id}:\")\n",
    "        print(f\"Topluluk boyutu: {data['size']}\")\n",
    "        print(\"Topluluk iÃ§eriÄŸi:\", \", \".join(data['keywords']))\n",
    "        print()\n",
    "\n",
    "# SÃ¼rÃ¼cÃ¼yÃ¼ kapat\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "from community import best_partition as community_best_partition\n",
    "\n",
    "# Neo4j baÄŸlantÄ±sÄ±\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi Ã§ekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# Networkx grafi oluÅŸturma\n",
    "G = nx.Graph()\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# Topluluk algÄ±sÄ±\n",
    "partition = community_best_partition(G)\n",
    "\n",
    "# SonuÃ§larÄ± yazdÄ±rma\n",
    "for node, community_id in partition.items():\n",
    "    print(f\"Node: {node}, Community ID: {community_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neo4j baÄŸlantÄ±sÄ±\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi Ã§ekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# NetworkX grafi oluÅŸturma\n",
    "G = nx.Graph()\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# GrafÄ± Ã§izme\n",
    "nx.draw(G, with_labels=True, node_size=1500, node_color=\"skyblue\", font_size=12, font_weight=\"bold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "\n",
    "# Neo4j baÄŸlantÄ±sÄ±\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi Ã§ekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# Networkx grafi oluÅŸturma\n",
    "G = nx.Graph()\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# GÃ¶rselleÅŸtirme\n",
    "print(\"DÃ¼ÄŸÃ¼m sayÄ±sÄ±:\", G.number_of_nodes())\n",
    "print(\"Kenar sayÄ±sÄ±:\", G.number_of_edges())\n",
    "plt.savefig(\"graf.png\")\n",
    "\n",
    "nx.draw(G, with_labels=True, node_size=1500, node_color=\"skyblue\", font_size=12, font_weight=\"bold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mevcut veri\n",
    "edges = [(\"A\", \"B\"), (\"B\", \"C\"), (\"B\", \"D\"), (\"D\", \"E\"), (\"D\", \"F\")]\n",
    "\n",
    "def create_graph(edges):\n",
    "    G = nx.Graph()\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    return G\n",
    "\n",
    "def plot_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=1000, font_size=10)\n",
    "    plt.title(\"Graf\")\n",
    "    plt.show()\n",
    "\n",
    "# Graf oluÅŸtur\n",
    "graph = create_graph(edges)\n",
    "\n",
    "# GrafÄ± Ã§iz\n",
    "plot_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Veriler\n",
    "data = {\n",
    "    \"Node1\": {\"links\": [\"Node2\", \"Node3\"]},\n",
    "    \"Node2\": {\"links\": [\"Node1\", \"Node4\"]},\n",
    "    \"Node3\": {\"links\": [\"Node1\", \"Node4\"]},\n",
    "    \"Node4\": {\"links\": [\"Node2\", \"Node3\", \"Node5\"]},\n",
    "    \"Node5\": {\"links\": [\"Node4\", \"Node6\"]},\n",
    "    \"Node6\": {\"links\": [\"Node5\"]}\n",
    "}\n",
    "\n",
    "\n",
    "def create_graph(data):\n",
    "    G = nx.Graph()\n",
    "    for node, node_data in data.items():\n",
    "        for link in node_data[\"links\"]:\n",
    "            G.add_edge(node, link)\n",
    "    return G\n",
    "\n",
    "def plot_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=1000, font_size=10)\n",
    "    plt.title(\"Graf\")\n",
    "    plt.show()\n",
    "\n",
    "# Graf oluÅŸtur\n",
    "graph = create_graph(data)\n",
    "\n",
    "# GrafÄ± Ã§iz\n",
    "plot_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "\n",
    "# Neo4j baÄŸlantÄ±sÄ±\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi Ã§ekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# Networkx grafi oluÅŸturma\n",
    "G = nx.DiGraph()  # YÃ¶nlÃ¼ graf\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# GÃ¶rselleÅŸtirme\n",
    "print(\"DÃ¼ÄŸÃ¼m sayÄ±sÄ±:\", G.number_of_nodes())\n",
    "print(\"Kenar sayÄ±sÄ±:\", G.number_of_edges())\n",
    "plt.savefig(\"graf.png\")\n",
    "\n",
    "nx.draw(G, with_labels=True, node_size=1500, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrows=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# BoÅŸ bir NetworkX grafÄ± oluÅŸtur\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Her bir web sayfasÄ± iÃ§in dÃ¼ÄŸÃ¼mleri ve Ã¶zelliklerini ekle\n",
    "for url, page_info in data.items():\n",
    "    # DÃ¼ÄŸÃ¼m Ã¶zelliklerini al\n",
    "    node_attributes = {}\n",
    "    if \"text\" in page_info:\n",
    "        node_attributes[\"text\"] = page_info[\"text\"]\n",
    "    if \"embeddings\" in page_info:\n",
    "        node_attributes[\"embeddings\"] = page_info[\"embeddings\"]\n",
    "    if \"keywords\" in page_info:\n",
    "        node_attributes[\"keywords\"] = page_info[\"keywords\"]\n",
    "\n",
    "    # DÃ¼ÄŸÃ¼mÃ¼ ekle\n",
    "    G.add_node(url, **node_attributes)\n",
    "\n",
    "    # BaÄŸlantÄ±larÄ± ekle\n",
    "    for link in page_info[\"links\"]:\n",
    "        G.add_edge(url, link)\n",
    "\n",
    "# Knowledge graph'Ä± gÃ¶rselleÅŸtir\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, pos, with_labels=True, node_size=1000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", arrowsize=10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
