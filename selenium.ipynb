{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install selenium==4.7.2 transformers==4.25.1 sentence-transformers===2.2.2 graphdatascience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = \"python programlama dili\"\n",
    "x = x[-6:-18:-5]\n",
    "print(x)\n",
    "print(\"{1}{0}{2}{4}\".format(5,4,1,2,3))\n",
    "from functools import reduce\n",
    "f=lambda a1,a2:a1*a2\n",
    "s=reduce(f,range(5))\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!apt-get update\n",
    "#!apt install chromium-chromedriver\n",
    "#!apt install -y xvfb\n",
    "\n",
    "%pip install undetected-chromedriver==3.1.7\n",
    "%pip install PyVirtualDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install undetected-chromedriver==3.1.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import undetected_chromedriver.v2 as uc\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "display = Display(visible=0, size=(800, 600))\n",
    "display.start()\n",
    "\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade jupyter ipywidgets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoModel\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanekyuk/bert-uncased-keyword-extractor\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"yanekyuk/bert-uncased-keyword-extractor\"\n",
    ")\n",
    "\n",
    "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "def extract_keywords(text):\n",
    "    \"\"\"\n",
    "    Extract keywords and construct them back from tokens\n",
    "    \"\"\"\n",
    "    result = list()\n",
    "    keyword = \"\"\n",
    "    for token in nlp(text):\n",
    "        if token[\"entity\"] == \"I-KEY\":\n",
    "            keyword += (\n",
    "                token[\"word\"][2:]\n",
    "                if token[\"word\"].startswith(\"##\")\n",
    "                else f\" {token['word']}\"\n",
    "            )\n",
    "        else:\n",
    "            if keyword:\n",
    "                result.append(keyword)\n",
    "            keyword = token[\"word\"]\n",
    "    # Add the last keyword\n",
    "    result.append(keyword)\n",
    "    return list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_keywords(\n",
    "    \"\"\"\n",
    "Broadcom agreed to acquire cloud computing company VMware in a $61 billion (€57bn) cash-and stock deal.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_embeddings(\n",
    "    \"\"\"\n",
    "Web APIs are a huge opportunity to access and integrate data from any sources with your graph. Most of them provide the data in JSON format.\n",
    "\n",
    "The Load JSON procedures retrieve data from URLs or maps and turn it into map value(s) for Cypher to consume. Cypher has support for deconstructing nested documents with dot syntax, slices, UNWIND etc. so it is easy to turn nested data into graphs.\n",
    "\n",
    "Sources with multiple JSON objects (JSONL,JSON Lines) in a stream, like the streaming Twitter format or the Yelp Kaggle dataset, are also supported,\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "    \n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]\n",
    "\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "\n",
    "entry_url = \"https://neo4j.com/docs\"\n",
    "data = dict()\n",
    "visit_list = [entry_url]\n",
    "already_visited = []\n",
    "\n",
    "while visit_list:\n",
    "    # Visit the URL\n",
    "    current_url = visit_list.pop()\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "    print(current_url)\n",
    "    try:\n",
    "        wd.get(current_url)\n",
    "    except:\n",
    "        print(f\"Couldn't open {current_url}\")\n",
    "        already_visited.append(current_url)\n",
    "        continue\n",
    "    # If redirect\n",
    "    try:\n",
    "        actual_url = wd.current_url.rstrip(\"/\").split(\"#\")[0].split(\"?\")[0]\n",
    "        if actual_url != current_url:\n",
    "            # Store the redirect information\n",
    "            data[current_url] = {\n",
    "                \"links\": [],\n",
    "                \"text\": None,\n",
    "                \"embeddings\": [],\n",
    "                \"keywords\": [],\n",
    "                \"redirects\": [actual_url],\n",
    "            }\n",
    "            already_visited.append(current_url)\n",
    "            # Content is being extracted from the actual url\n",
    "            current_url = actual_url\n",
    "    except:\n",
    "        pass\n",
    "    # Extract text from the content div\n",
    "    text = extract_text_by_class(\"content\")\n",
    "    # If nothing is found, try article div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"article\")\n",
    "    # If nothing is found, try page div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"page\")\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"single-user-story\")\n",
    "    # Check if 404\n",
    "    try:\n",
    "        if \"Sorry, page not found\" in wd.find_element(By.TAG_NAME, \"body\").text:\n",
    "            text = \"404\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Generate paragraph embedding & extract keywords\n",
    "    if text:\n",
    "        embeddings = generate_embeddings(text)\n",
    "        keywords = extract_keywords(text)\n",
    "    else:\n",
    "        embeddings = []\n",
    "        keywords = []\n",
    "\n",
    "    # Extract links from the content div\n",
    "    links = extract_links_by_xpath(\"//div[@class='content']//a[@href]\")\n",
    "    # If nothing is found, try article div\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article[@class='article']//a[@href]\")\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article//a[@href]\")\n",
    "\n",
    "    # if not links and not text:\n",
    "    #    print(f\"Couldn't retrieve the data from {current_url}\")\n",
    "    # Store page information\n",
    "    data[current_url] = {\n",
    "        \"links\": [l for l in links if l != current_url],\n",
    "        \"text\": text,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"keywords\": keywords,\n",
    "        \"redirects\": [],\n",
    "    }\n",
    "    # Crawling information\n",
    "    already_visited.append(current_url)\n",
    "    # Don't leave neo4j.com while crawling\n",
    "    # and avoid community and sandbox\n",
    "    visit_list.extend(\n",
    "        [\n",
    "            l\n",
    "            for l in list(links)\n",
    "            if (\"neo4j.com\" in l)\n",
    "            and (not l in already_visited)\n",
    "            and (not \"community.neo4j.com\" in l)\n",
    "            and (not \"sandbox.neo4j.com\" in l)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # service.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def extract_text_by_class(class_name):\n",
    "    \"\"\"\n",
    "    Extract text from an element with the specified class name\n",
    "    \"\"\"\n",
    "    global wd\n",
    "    try:\n",
    "        content = wd.find_element(By.CLASS_NAME, class_name)\n",
    "        return content.text\n",
    "    except:\n",
    "        return \"\"\n",
    "\n",
    "def extract_links_by_xpath(xpath):\n",
    "    global wd\n",
    "    links = set()\n",
    "    try:\n",
    "        a_elems = wd.find_elements(By.XPATH, xpath)\n",
    "        for elem in a_elems:\n",
    "            link = elem.get_attribute(\"href\")\n",
    "            if link == \"javascript:void(0)\":\n",
    "                continue\n",
    "            # Remove links to images and various files\n",
    "            if (\n",
    "                link.endswith(\".png\")\n",
    "                or link.endswith(\".json\")\n",
    "                or link.endswith(\".txt\")\n",
    "                or link.endswith(\".svg\")\n",
    "                or link.endswith(\".ipynb\")\n",
    "                or link.endswith(\".jpg\")\n",
    "                or link.endswith(\".pdf\")\n",
    "                or link.endswith(\".mp4\")\n",
    "                or \"mailto\" in link\n",
    "                or len(link) > 300\n",
    "            ):\n",
    "                continue\n",
    "            # Remove anchors\n",
    "            link = link.split(\"#\")[0]\n",
    "            # Remove parameters\n",
    "            link = link.split(\"?\")[0]\n",
    "            # Remove trailing forward slash\n",
    "            link = link.rstrip(\"/\")\n",
    "            links.add(link)\n",
    "        return list(links)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    embeddings = model.encode(text)\n",
    "    return [float(x) for x in embeddings.tolist()]\n",
    "\n",
    "# Set up the Chrome webdriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "wd = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "# Set up the SentenceTransformer model\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "entry_url = \"https://neo4j.com/docs\"\n",
    "data = dict()\n",
    "visit_list = [entry_url]\n",
    "already_visited = []\n",
    "visited_links_count = 0  # Sayaç\n",
    "\n",
    "while visit_list and visited_links_count < 20:  # 100'e ulaşana kadar devam et\n",
    "    # Visit the URL\n",
    "    current_url = visit_list.pop()\n",
    "    if current_url in already_visited:\n",
    "        continue\n",
    "    print(current_url)\n",
    "    try:\n",
    "        wd.get(current_url)\n",
    "    except:\n",
    "        print(f\"Couldn't open {current_url}\")\n",
    "        already_visited.append(current_url)\n",
    "        continue\n",
    "    # If redirect\n",
    "    try:\n",
    "        actual_url = wd.current_url.rstrip(\"/\").split(\"#\")[0].split(\"?\")[0]\n",
    "        if actual_url != current_url:\n",
    "            # Store the redirect information\n",
    "            data[current_url] = {\n",
    "                \"links\": [],\n",
    "                \"text\": None,\n",
    "                \"embeddings\": [],\n",
    "                \"keywords\": [],\n",
    "                \"redirects\": [actual_url],\n",
    "            }\n",
    "            already_visited.append(current_url)\n",
    "            # Content is being extracted from the actual url\n",
    "            current_url = actual_url\n",
    "    except:\n",
    "        pass\n",
    "    # Extract text from the content div\n",
    "    text = extract_text_by_class(\"content\")\n",
    "    # If nothing is found, try article div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"article\")\n",
    "    # If nothing is found, try page div\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"page\")\n",
    "    if not text:\n",
    "        text = extract_text_by_class(\"single-user-story\")\n",
    "    # Check if 404\n",
    "    try:\n",
    "        if \"Sorry, page not found\" in wd.find_element(By.TAG_NAME, \"body\").text:\n",
    "            text = \"404\"\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Generate paragraph embedding & extract keywords\n",
    "    if text:\n",
    "        embeddings = generate_embeddings(text)\n",
    "        keywords = extract_keywords(text)\n",
    "    else:\n",
    "        embeddings = []\n",
    "        keywords = []\n",
    "\n",
    "    # Extract links from the content div\n",
    "    links = extract_links_by_xpath(\"//div[@class='content']//a[@href]\")\n",
    "    # If nothing is found, try article div\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article[@class='article']//a[@href]\")\n",
    "    if not links:\n",
    "        links = extract_links_by_xpath(\"//article//a[@href]\")\n",
    "\n",
    "    # if not links and not text:\n",
    "    #    print(f\"Couldn't retrieve the data from {current_url}\")\n",
    "    # Store page information\n",
    "    data[current_url] = {\n",
    "        \"links\": [l for l in links if l != current_url],\n",
    "        \"text\": text,\n",
    "        \"embeddings\": embeddings,\n",
    "        \"keywords\": keywords,\n",
    "        \"redirects\": [],\n",
    "    }\n",
    "    # Crawling information\n",
    "    already_visited.append(current_url)\n",
    "    visited_links_count += 1  # Her ziyarette sayaç arttırılıyor\n",
    "    # Don't leave neo4j.com while crawling\n",
    "    # and avoid community and sandbox\n",
    "    visit_list.extend(\n",
    "        [\n",
    "            l\n",
    "            for l in list(links)\n",
    "            if (\"neo4j.com\" in l)\n",
    "            and (not l in already_visited)\n",
    "            and (not \"community.neo4j.com\" in l)\n",
    "            and (not \"sandbox.neo4j.com\" in l)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Close the webdriver\n",
    "wd.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://5a2c45ba.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"cMVJrBii27ZJMgsEBm5gw-uE7e2rGXnRHk1MFLFlu5c\"\n",
    "\n",
    "try:\n",
    "    driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "    with driver.session() as session:\n",
    "        result = session.run(\"RETURN 1\")\n",
    "        for record in result:\n",
    "            print(record)\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "j = json.dumps(data)\n",
    "# open file for writing, \"w\"\n",
    "f = open(\"neo4j_docs.json\", \"w\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(j)\n",
    "\n",
    "# close file\n",
    "f.close()\n",
    "# Import to Neo4j\n",
    "from graphdatascience import GraphDataScience\n",
    "\n",
    "host = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "gds = GraphDataScience(host, auth=(user, password))\n",
    "\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT IF NOT EXISTS FOR (p:Page) REQUIRE p.url IS UNIQUE;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "gds.run_cypher(\n",
    "    \"\"\"\n",
    "CREATE CONSTRAINT IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE;\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Veritabanı bağlantısını oluştur\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "# Veritabanına yazma işlemlerini gerçekleştir\n",
    "with driver.session() as session:\n",
    "    # Json verilerini dosyaya yaz\n",
    "    j = json.dumps(data)\n",
    "    with open(\"neo4j_docs.json\", \"w\") as f:\n",
    "        f.write(j)\n",
    "\n",
    "    # Kısıtlamaları oluştur\n",
    "    session.run(\"\"\"\n",
    "        CREATE CONSTRAINT IF NOT EXISTS FOR (p:Page) REQUIRE p.url IS UNIQUE;\n",
    "    \"\"\")\n",
    "    \n",
    "    session.run(\"\"\"\n",
    "        CREATE CONSTRAINT IF NOT EXISTS FOR (k:Keyword) REQUIRE k.name IS UNIQUE;\n",
    "    \"\"\")\n",
    "\n",
    "# Veritabanı bağlantısını kapat\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import_query = \"\"\"\n",
    "\n",
    "UNWIND $data AS row\n",
    "MERGE (p:Page {url:row.url})\n",
    "SET p.embedding = row.embedding,\n",
    "    p.has_text = row.has_text,\n",
    "    p.is_404 = row.is_404\n",
    "FOREACH (l in row.links    | MERGE (p1:Page {url:l}) MERGE (p)-[:LINKS_TO]->(p1))\n",
    "FOREACH (k in row.keywords | MERGE (k1:Keyword {name:k}) MERGE (p)-[:HAS_KEYWORD]->(k1))\n",
    "FOREACH (r in row.redirects| MERGE (r1:Page {url:r}) MERGE (p)-[:REDIRECTS]->(r1))\n",
    "\n",
    "\"\"\"\n",
    "x = 1\n",
    "params = []\n",
    "for key in data:\n",
    "    params.append(\n",
    "        {\n",
    "            \"url\": key,\n",
    "            \"embedding\": data[key][\"embeddings\"],\n",
    "            \"keywords\": data[key][\"keywords\"],\n",
    "            \"links\": data[key][\"links\"],\n",
    "            \"has_text\": True if data[key][\"text\"] else False,\n",
    "            \"is_404\": True if data[key][\"text\"] == \"404\" else False,\n",
    "            \"redirects\": data[key][\"redirects\"],\n",
    "        }\n",
    "    )\n",
    "    # Batch per 500\n",
    "    if len(params) == 500:\n",
    "        gds.run_cypher(import_query, {\"data\": params})\n",
    "        params = []\n",
    "        # Logging\n",
    "        print(f\"Importing {x} batch\")\n",
    "        x += 1\n",
    "\n",
    "# Import the remainder\n",
    "gds.run_cypher(import_query, {\"data\": params})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Import to Neo4j\n",
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "import_query = \"\"\"\n",
    "UNWIND $data AS row\n",
    "MERGE (p:Page {url:row.url})\n",
    "SET p.embedding = row.embedding,\n",
    "    p.has_text = row.has_text,\n",
    "    p.is_404 = row.is_404\n",
    "FOREACH (l in row.links    | MERGE (p1:Page {url:l}) MERGE (p)-[:LINKS_TO]->(p1))\n",
    "FOREACH (k in row.keywords | MERGE (k1:Keyword {name:k}) MERGE (p)-[:HAS_KEYWORD]->(k1))\n",
    "FOREACH (r in row.redirects| MERGE (r1:Page {url:r}) MERGE (p)-[:REDIRECTS]->(r1))\n",
    "\"\"\"\n",
    "\n",
    "x = 1\n",
    "params = []\n",
    "\n",
    "for key in data:\n",
    "    params.append(\n",
    "        {\n",
    "            \"url\": key,\n",
    "            \"embedding\": data[key][\"embeddings\"],\n",
    "            \"keywords\": data[key][\"keywords\"],\n",
    "            \"links\": data[key][\"links\"],\n",
    "            \"has_text\": True if data[key][\"text\"] else False,\n",
    "            \"is_404\": True if data[key][\"text\"] == \"404\" else False,\n",
    "            \"redirects\": data[key][\"redirects\"],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Batch per 500\n",
    "    if len(params) == 500:\n",
    "        with driver.session() as session:\n",
    "            session.run(import_query, {\"data\": params})\n",
    "            params = []\n",
    "            # Logging\n",
    "            print(f\"Importing {x} batch\")\n",
    "            x += 1\n",
    "\n",
    "# Import the remainder\n",
    "with driver.session() as session:\n",
    "    session.run(import_query, {\"data\": params})\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "CALL apoc.meta.stats()\n",
    "YIELD labels, relTypesCount\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"labels\"], record[\"relTypesCount\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)\n",
    "RETURN p.has_text AS has_text,\n",
    "       count(*) AS count\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"has_text\"], record[\"count\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)\n",
    "WHERE p.has_text IS NULL\n",
    "WITH p, [(p)<-[:LINKS_TO|REDIRECTS]-() | 1] AS links\n",
    "RETURN p.url AS page, size(links) AS links_count\n",
    "ORDER BY links_count DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"page\"], record[\"links_count\"])\n",
    "\n",
    "driver.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GDS version\n",
    "# \n",
    "#  gds.run_cypher(\"\"\"\n",
    "#  MATCH (:Page)-[:LINKS_TO|REDIRECTS]->(:Page{is_404:true})\n",
    "#  RETURN count(*) AS brokenLinkCount\n",
    "#  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (:Page)-[:LINKS_TO|REDIRECTS]->(:Page{is_404:true})\n",
    "RETURN count(*) AS brokenLinkCount\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"brokenLinkCount\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (start:Page {url:\"https://neo4j.com/docs\"}), \n",
    "      (end:Page {url:\"https://console.neo4j.io\"})\n",
    "MATCH p=shortestPath((start)-[:LINKS_TO|REDIRECTS*..10]->(end))\n",
    "RETURN [n in nodes(p) | n.url] AS path\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"path\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install GraphProjector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import GraphProjector\n",
    "\n",
    "# GDS grafiğini oluştur\n",
    "graph_projector = GraphProjector(host, auth=(user, password))\n",
    "G = graph_projector.project_graph(\n",
    "    graph=\"structure\",\n",
    "    node_label=\"Page\",\n",
    "    relationship_types=[\"LINKS_TO\", \"REDIRECTS\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install graphdatascience\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import Degree\n",
    "\n",
    "# Ters yönlü dereceleri hesapla\n",
    "degree = Degree()\n",
    "df = degree.fit_transform(G, orientation=\"REVERSE\")\n",
    "\n",
    "# Her düğümün URL'sini al\n",
    "df[\"url\"] = [node[\"url\"] for node in gds.util.asNodes(df[\"nodeId\"].tolist())]\n",
    "\n",
    "# Dereceye göre DataFrame'i sırala\n",
    "df.sort_values(\"score\", ascending=False, inplace=True)\n",
    "\n",
    "# İlk beş öğeyi göster\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (n:Page)\n",
    "RETURN n.url AS url, size((n)<-[:LINKS_TO]-()) AS inDegree\n",
    "ORDER BY inDegree DESC\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"url\"], record[\"inDegree\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graphdatascience import PageRank\n",
    "\n",
    "# PageRank skorlarını hesapla\n",
    "pagerank = PageRank()\n",
    "pr_df = pagerank.fit_transform(G)\n",
    "\n",
    "# Skor sütununu \"pagerank\" olarak yeniden adlandır\n",
    "pr_df.rename(columns={\"score\": \"pagerank\"}, inplace=True)\n",
    "\n",
    "# DataFrame'leri birleştir\n",
    "combined_df = df.merge(pr_df, on=\"nodeId\")\n",
    "\n",
    "# PageRank'a göre sırala\n",
    "combined_df.sort_values(\"pagerank\", ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "CALL gds.pageRank.stream('structure') YIELD nodeId, score\n",
    "RETURN gds.util.asNode(nodeId).url AS url, score\n",
    "ORDER BY score DESC\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"url\"], record[\"score\"])\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (k:Keyword)\n",
    "RETURN k.name AS keyword,\n",
    "       COUNT(*) AS mentions\n",
    "ORDER BY mentions DESC\n",
    "LIMIT 5\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"keyword\"], record[\"mentions\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, metadata = gds.graph.project(\n",
    "    \"keywords\", [\"Page\", \"Keyword\"], {\"HAS_KEYWORD\": {\"orientation\": \"REVERSE\"}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (p:Page)-[:HAS_KEYWORD]->(k:Keyword)\n",
    "RETURN p.url AS page,\n",
    "       COLLECT(k.name) AS keywords\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    result = session.run(query)\n",
    "    for record in result:\n",
    "        print(record[\"page\"], record[\"keywords\"])\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Page), (b:Page)\n",
    "WHERE id(a) < id(b) AND NOT (a)-[:CO_OCCUR]->(b)\n",
    "WITH a, b, gds.alpha.similarity.jaccard([a, b], {relationshipQuery: \"CO_OCCUR\"}) AS similarity\n",
    "WHERE similarity > 0.4\n",
    "MERGE (a)-[r:CO_OCCUR {score: similarity}]->(b)\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(query)\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"\n",
    "user = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (a:Page), (b:Page)\n",
    "WHERE id(a) < id(b)\n",
    "WITH a, b,\n",
    "     algo.similarity.jaccard(apoc.coll.toSet(COLLECT(id(a))), apoc.coll.toSet(COLLECT(id(b)))) AS similarity\n",
    "WHERE similarity > 0.4 AND NOT (a)-[:CO_OCCUR]->(b)\n",
    "MERGE (a)-[r:CO_OCCUR {score: similarity}]->(b)\n",
    "\"\"\"\n",
    "\n",
    "with driver.session() as session:\n",
    "    session.run(query)\n",
    "\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = gds.louvain.stream(G, nodeLabels=[\"Keyword\"], relationshipTypes=[\"CO_OCCUR\"])\n",
    "topic_df[\"keyword\"] = [\n",
    "    n[\"name\"] for n in gds.util.asNodes(topic_df[\"nodeId\"].to_list())\n",
    "]\n",
    "topic_df.groupby(\"communityId\").agg(\n",
    "    {\"keyword\": [\"size\", list]}\n",
    ").reset_index().sort_values([(\"keyword\", \"size\")], ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_df = gds.louvain.stream(G, tolerance=(0.00001), nodeLabels=[\"Keyword\"], relationshipTypes=[\"CO_OCCUR\"])\n",
    "topic_df[\"keyword\"] = [\n",
    "    n[\"name\"] for n in gds.util.asNodes(topic_df[\"nodeId\"].to_list())\n",
    "]\n",
    "topic_df.groupby(\"communityId\").agg(\n",
    "    {\"keyword\": [\"size\", list]}\n",
    ").reset_index().sort_values([(\"keyword\", \"size\")], ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import community\n",
    "\n",
    "# Create a network (you can replace this with your actual network data)\n",
    "graph = {\n",
    "  0: [1, 2],\n",
    "  1: [0, 2, 3],\n",
    "  2: [0, 1, 3, 4],\n",
    "  3: [1, 2, 4],\n",
    "  4: [2, 3]\n",
    "}\n",
    "\n",
    "# Detect communities using Louvain method\n",
    "partition = community.best_partition(graph)\n",
    "\n",
    "# Print the communities\n",
    "for node, comm in partition.items():\n",
    "  print(f\"Node {node} belongs to community {comm}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "\n",
    "# Neo4j veritabanına bağlan\n",
    "uri = \"neo4j+s://76cc1e9c.databases.neo4j.io\"  # Bağlanmak istediğiniz veritabanının URI'si\n",
    "username = \"neo4j\"\n",
    "password = \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"\n",
    "\n",
    "# Oturumu oluştur\n",
    "driver = GraphDatabase.driver(uri, auth=(username, password))\n",
    "\n",
    "def louvain_community_detection(session):\n",
    "    # Topluluk algısını gerçekleştirme sorgusu\n",
    "    query = \"\"\"\n",
    "    CALL gds.louvain.stream('myGraph') \n",
    "    YIELD nodeId, communityId\n",
    "    RETURN gds.util.asNode(nodeId).name AS name, communityId\n",
    "    \"\"\"\n",
    "    result = session.run(query)\n",
    "    community_data = {record[\"name\"]: record[\"communityId\"] for record in result}\n",
    "    return community_data\n",
    "\n",
    "def convert_node_ids_to_keywords(session, community_data):\n",
    "    # Anahtar kelime kimliklerini anahtar kelime adlarına dönüştürme sorgusu\n",
    "    query = \"\"\"\n",
    "    MATCH (k:Keyword)\n",
    "    WHERE id(k) IN $nodeIds\n",
    "    RETURN k.name AS name\n",
    "    \"\"\"\n",
    "    node_ids = [int(node_id) for node_id in community_data.keys()]\n",
    "    result = session.run(query, {\"nodeIds\": node_ids})\n",
    "    keyword_data = {record[\"name\"]: community_data[str(node_id)] for record, node_id in zip(result, node_ids)}\n",
    "    return keyword_data\n",
    "\n",
    "def analyze_communities(community_data, keyword_data):\n",
    "    # Topluluk analizini gerçekleştirme\n",
    "    community_keyword_count = {}\n",
    "    for keyword, community_id in keyword_data.items():\n",
    "        if community_id not in community_keyword_count:\n",
    "            community_keyword_count[community_id] = {\"size\": 0, \"keywords\": []}\n",
    "        community_keyword_count[community_id][\"size\"] += 1\n",
    "        community_keyword_count[community_id][\"keywords\"].append(keyword)\n",
    "    return community_keyword_count\n",
    "\n",
    "# Grafa erişim sağla ve işlemleri gerçekleştir\n",
    "with driver.session() as session:\n",
    "    # Topluluk algısını gerçekleştir\n",
    "    community_data = louvain_community_detection(session)\n",
    "\n",
    "    # Anahtar kelime kimliklerini anahtar kelime adlarına dönüştür\n",
    "    keyword_data = convert_node_ids_to_keywords(session, community_data)\n",
    "\n",
    "    # Topluluk analizini gerçekleştir\n",
    "    community_keyword_count = analyze_communities(community_data, keyword_data)\n",
    "\n",
    "    # Sonuçları işle\n",
    "    sorted_communities = sorted(community_keyword_count.items(), key=lambda x: x[1][\"size\"], reverse=True)\n",
    "    top_communities = sorted_communities[:5]  # İlk 5 büyük topluluğu al\n",
    "\n",
    "    # Toplulukları ve içerdiği anahtar kelimeleri yazdır\n",
    "    for community_id, data in top_communities:\n",
    "        print(f\"Topluluk {community_id}:\")\n",
    "        print(f\"Topluluk boyutu: {data['size']}\")\n",
    "        print(\"Topluluk içeriği:\", \", \".join(data['keywords']))\n",
    "        print()\n",
    "\n",
    "# Sürücüyü kapat\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "from community import best_partition as community_best_partition\n",
    "\n",
    "# Neo4j bağlantısı\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi çekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# Networkx grafi oluşturma\n",
    "G = nx.Graph()\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# Topluluk algısı\n",
    "partition = community_best_partition(G)\n",
    "\n",
    "# Sonuçları yazdırma\n",
    "for node, community_id in partition.items():\n",
    "    print(f\"Node: {node}, Community ID: {community_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neo4j bağlantısı\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi çekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# NetworkX grafi oluşturma\n",
    "G = nx.Graph()\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# Grafı çizme\n",
    "nx.draw(G, with_labels=True, node_size=1500, node_color=\"skyblue\", font_size=12, font_weight=\"bold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "\n",
    "# Neo4j bağlantısı\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi çekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# Networkx grafi oluşturma\n",
    "G = nx.Graph()\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# Görselleştirme\n",
    "print(\"Düğüm sayısı:\", G.number_of_nodes())\n",
    "print(\"Kenar sayısı:\", G.number_of_edges())\n",
    "plt.savefig(\"graf.png\")\n",
    "\n",
    "nx.draw(G, with_labels=True, node_size=1500, node_color=\"skyblue\", font_size=12, font_weight=\"bold\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Mevcut veri\n",
    "edges = [(\"A\", \"B\"), (\"B\", \"C\"), (\"B\", \"D\"), (\"D\", \"E\"), (\"D\", \"F\")]\n",
    "\n",
    "def create_graph(edges):\n",
    "    G = nx.Graph()\n",
    "    for edge in edges:\n",
    "        G.add_edge(edge[0], edge[1])\n",
    "    return G\n",
    "\n",
    "def plot_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=1000, font_size=10)\n",
    "    plt.title(\"Graf\")\n",
    "    plt.show()\n",
    "\n",
    "# Graf oluştur\n",
    "graph = create_graph(edges)\n",
    "\n",
    "# Grafı çiz\n",
    "plot_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Veriler\n",
    "data = {\n",
    "    \"Node1\": {\"links\": [\"Node2\", \"Node3\"]},\n",
    "    \"Node2\": {\"links\": [\"Node1\", \"Node4\"]},\n",
    "    \"Node3\": {\"links\": [\"Node1\", \"Node4\"]},\n",
    "    \"Node4\": {\"links\": [\"Node2\", \"Node3\", \"Node5\"]},\n",
    "    \"Node5\": {\"links\": [\"Node4\", \"Node6\"]},\n",
    "    \"Node6\": {\"links\": [\"Node5\"]}\n",
    "}\n",
    "\n",
    "\n",
    "def create_graph(data):\n",
    "    G = nx.Graph()\n",
    "    for node, node_data in data.items():\n",
    "        for link in node_data[\"links\"]:\n",
    "            G.add_edge(node, link)\n",
    "    return G\n",
    "\n",
    "def plot_graph(graph):\n",
    "    pos = nx.spring_layout(graph)\n",
    "    nx.draw(graph, pos, with_labels=True, node_size=1000, font_size=10)\n",
    "    plt.title(\"Graf\")\n",
    "    plt.show()\n",
    "\n",
    "# Graf oluştur\n",
    "graph = create_graph(data)\n",
    "\n",
    "# Grafı çiz\n",
    "plot_graph(graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "import networkx as nx\n",
    "\n",
    "# Neo4j bağlantısı\n",
    "graph = Graph(uri=\"neo4j+s://76cc1e9c.databases.neo4j.io\", auth=(\"neo4j\", \"nxPD_cEI0UMmTyBopgn5HZncmGcxuHNLuGzEJVy_4s0\"))\n",
    "\n",
    "# Grafi çekme\n",
    "query = \"\"\"\n",
    "MATCH (n)-[r]->(m)\n",
    "RETURN n, r, m\n",
    "\"\"\"\n",
    "result = graph.run(query)\n",
    "\n",
    "# Networkx grafi oluşturma\n",
    "G = nx.DiGraph()  # Yönlü graf\n",
    "for record in result:\n",
    "    node1_name = record[\"n\"][\"name\"]\n",
    "    node2_name = record[\"m\"][\"name\"]\n",
    "    if node1_name is not None and node2_name is not None:\n",
    "        G.add_edge(node1_name, node2_name)\n",
    "\n",
    "# Görselleştirme\n",
    "print(\"Düğüm sayısı:\", G.number_of_nodes())\n",
    "print(\"Kenar sayısı:\", G.number_of_edges())\n",
    "plt.savefig(\"graf.png\")\n",
    "\n",
    "nx.draw(G, with_labels=True, node_size=1500, node_color=\"skyblue\", font_size=12, font_weight=\"bold\", arrows=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Boş bir NetworkX grafı oluştur\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Her bir web sayfası için düğümleri ve özelliklerini ekle\n",
    "for url, page_info in data.items():\n",
    "    # Düğüm özelliklerini al\n",
    "    node_attributes = {}\n",
    "    if \"text\" in page_info:\n",
    "        node_attributes[\"text\"] = page_info[\"text\"]\n",
    "    if \"embeddings\" in page_info:\n",
    "        node_attributes[\"embeddings\"] = page_info[\"embeddings\"]\n",
    "    if \"keywords\" in page_info:\n",
    "        node_attributes[\"keywords\"] = page_info[\"keywords\"]\n",
    "\n",
    "    # Düğümü ekle\n",
    "    G.add_node(url, **node_attributes)\n",
    "\n",
    "    # Bağlantıları ekle\n",
    "    for link in page_info[\"links\"]:\n",
    "        G.add_edge(url, link)\n",
    "\n",
    "# Knowledge graph'ı görselleştir\n",
    "pos = nx.spring_layout(G, k=0.15, iterations=20)\n",
    "plt.figure(figsize=(12, 8))\n",
    "nx.draw(G, pos, with_labels=True, node_size=1000, node_color=\"skyblue\", font_size=10, font_weight=\"bold\", arrowsize=10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
